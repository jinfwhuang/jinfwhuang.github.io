<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="jin" />

        <meta name="description" content="ChatGPT has become one of the most popular AI assistants. It is a fixed, general purpose model. That is by design because it learns from mostly texts in the public domain. This applies to open source LLMs as well, e.g. T5, LLaMA, and falcon. However, there are many reasons …
" />
        <meta name="twitter:creator" content="@jinfwhuang">
        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="ai, LLM, misc, " />

<meta property="og:title" content="Domain Specific AI Assistants "/>
<meta property="og:url" content="/2023-06-04-domain-specific-ai-assistant" />
<meta property="og:description" content="ChatGPT has become one of the most popular AI assistants. It is a fixed, general purpose model. That is by design because it learns from mostly texts in the public domain. This applies to open source LLMs as well, e.g. T5, LLaMA, and falcon. However, there are many reasons …" />
<meta property="og:site_name" content="Jin&#39;s Notes" />
<meta property="og:article:author" content="jin" />
<meta property="og:article:published_time" content="2023-06-04T00:00:00-07:00" />
<meta name="twitter:title" content="Domain Specific AI Assistants ">
<meta name="twitter:description" content="ChatGPT has become one of the most popular AI assistants. It is a fixed, general purpose model. That is by design because it learns from mostly texts in the public domain. This applies to open source LLMs as well, e.g. T5, LLaMA, and falcon. However, there are many reasons …">
<meta property="og:image" content="/images/android-chrome-192x192.png" />
<meta name="twitter:image" content="/images/android-chrome-192x192.png" >

        <title>Domain Specific AI Assistants  · Jin&#39;s Notes
</title>
        <link rel="shortcut icon" href="/theme/images/favicon.ico" type="image/x-icon" />
        <link rel="icon" href="/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="/theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="/theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="/theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="/theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="/theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="/theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="/theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="/theme/images/apple-touch-icon-180x180.png" type="image/png" />
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-207279664-1', 'auto');
    ga('send', 'pageview');
</script>


    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="/"><span class=site-name><span style="color:black;">Jin's Notes</span></span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       "/"
                                    >Home</a>
                                </li>
<!--                                <li ><a href="/categories">Categories</a></li>-->
                                <li ><a href="/tags">Tags</a></li>
                                <li ><a href="/archives">Archives</a></li>
                                <li><form class="navbar-search" action="/search" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="/2023-06-04-domain-specific-ai-assistant">
                Domain Specific <span class="caps">AI</span> Assistants <br/>
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
    <div class="span2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div class="toc">
<ul>
<li><a href="#knowledge-infusion-through-model-training">Knowledge Infusion Through Model Training</a></li>
<li><a href="#llm-information-extraction-techniques"><span class="caps">LLM</span> Information Extraction Techniques</a></li>
<li><a href="#retrieval-augmentation">Retrieval-Augmentation</a></li>
<li><a href="#external-tools">External Tools</a></li>
<li><a href="#final-thoughts">Final Thoughts</a></li>
<li><a href="#footnotes">Footnotes</a></li>
</ul>
</div>
        </nav>
    </div>
    <div class="span8 article-content">
            
            
<p>ChatGPT has become one of the most popular <span class="caps">AI</span> assistants. It is a fixed, general purpose model. That is by design because it learns from mostly texts in the public domain. This applies to open source LLMs as well, e.g. <a href="https://huggingface.co/docs/transformers/model_doc/flan-t5">T5</a>, <a href="https://huggingface.co/docs/transformers/main/model_doc/llama">LLaMA</a>, and <a href="https://huggingface.co/tiiuae/falcon-40b">falcon</a>. However, there are many reasons to build <span class="caps">AI</span> assistants that specialize in limited knowledge domains. For example, I might want to talk an <span class="caps">AI</span> assistant specific to home purchasing, an assistant is limited by a company’s private corpus of documents, or a personal assistant that has access to my private emails, texts, and calendar. In this post, I will discuss the techniques relevant to building a domain-specific <span class="caps">AI</span> assistant.</p>
<p>An <span class="caps">LLM</span> accesses private information in one of two ways. The information could be baked into model parameters, and then the chatbot retrieves relevant information through prompting. This requires the <span class="caps">LLM</span> to be trained on private data after it acquires its general comprehension capabilities. The other way is to provide the domain specific knowledge as input context. The information retrieval step is performed outside of the language model. It could be calls to external APIs or searches of a document store.</p>
<h4 id="knowledge-infusion-through-model-training">Knowledge Infusion Through Model Training</h4>
<p>LLMs could answer a wide range of common questions directly. Their breadth of knowledge could be comparable to a search engine. LLMs memorize information by applying some pretraining objectives to a large corpus of data. The data sources are usually a mixture of internet text, books, and other various public text corpus.</p>
<p>It is important to note that an <span class="caps">LLM</span> only loosely absorbs information from the training corpus. There is no guarantee that any one piece of data will be 100% burned into model parameters. Even the largest neural model is not likely to incorporate information in any predictable or uniform way. There are estimates about those information retention rates. For example, <a href='#carlini2023quantifying' id='ref-carlini2023quantifying-1'>
    2
</a> quantifies the some lower bound to be <code>1%</code>. It is impossible to exactly quantify this retention rate. The memorization mechanism is hard to know for sure. We only know that a large neural model could learn from simple pretraining objectives. <a href='#porada2021does' id='ref-porada2021does-1'>
    5
</a> discusses the limit of knowledge acquisition through masked language pretraining objectives. We are still early in our understanding of how LLMs work and how to properly perform precise memory infusion, extraction, or modification.</p>
<p>There are many unsupervised training techniques to infuse knowledge and capabilities to LLMs. The most common is the various forms of masking spans. For example, next token prediction, next sentence, and random masked spans are the most popular and common pretraining objectives. 
<figure>
<img src="images/2023-06-training-span-corruption.png"/>
<figcaption align="center">
Pretraining objectives: different span corruption strategies (<a href='#tay2023ul2' id='ref-tay2023ul2-1'>
    10
</a>)
</figcaption>
</figure></p>
<p>If the private domain contains structured knowledge, e.g. knowledge triplets, one could devise a masking strategy that focuses on key concepts and relationships (<a href='#sun2021ernie' id='ref-sun2021ernie-1'>
    8
</a> and <a href='#moiseev2022skill' id='ref-moiseev2022skill-1'>
    3
</a>). 
<figure>
<img src="images/2023-06-training-knowledge-triplet-objective.png" width="80%"/>
<img src="images/2023-06-training-word-aware-task.png" width="90%"/>
<figcaption align="center">
 Pretraining objectives: structured knowledge infusion and universal knowledge-text prediction
</figcaption>
</figure></p>
<p>Other objectives could be predicting sentence ordering. The model is asked to recover the correct ordering from a randomly permuted sentence order. Another objective could be sentence distance. A model takes two sentences as input and predict the category in the set of adjacent, nonadjacent but same document, and not in the same document.</p>
<p>Other than using unsupervised training objectives, it is also possible to update the model parameters through the use of synthetic data with a task-specific fine-tuning objective. Some LLMs, either the one undertraining or an external <span class="caps">LLM</span>, could be used to generate input and target pairs from raw texts. The input and target pairs could be in the form of question and answers, question and supporting passages, queries and topic classifications, etc. The model is then trained on these labeled data.
<figure>
<img align="middle" src="images/2023-06-training-synthetic-fine-tune-data.png" width="50%"/>
<figcaption align="center">
 Fine-tuning on synthetic labeled data (<a href='#saadfalcon2023udapdr' id='ref-saadfalcon2023udapdr-1'>
    6
</a>)
</figcaption>
</figure></p>
<h4 id="llm-information-extraction-techniques"><span class="caps">LLM</span> Information Extraction Techniques</h4>
<p>There are different techniques to extract information from model parameters. The most obvious approach is through prompt engineering. The most well-known prompt techniques are few-shot learning and chain-of-throught prompting. </p>
<figure>
<img align="left" src="images/2023-06-prompt-few-shots.png" width="50%"/>
<img src="images/2023-06-prompt-chain-of-thought.png" width="50%"/>
<figcaption align="center">
 Prompting: few-shot (<a href='#brown2020language' id='ref-brown2020language-1'>
    1
</a>) and chain of thought (<a href='#wei2023chainofthought' id='ref-wei2023chainofthought-1'>
    12
</a>)
</figcaption>
</figure>
<p>There are additional variations. For example, the few-shot could be combined with synthetic example generation. The few examples could be modified to be a recitation-augmented prompt, as proposed by <a href='#sun2023recitationaugmented' id='ref-sun2023recitationaugmented-1'>
    9
</a>.
<figure>
<img src="images/2023-06-prompt-recitation.png" width="50%"/>
<figcaption align="center">
 Prompting: few-shot with recitations
</figcaption>
</figure></p>
<p>Other than better prompts, there are other ways to improve information extraction from model parameters. <a href='#wang2023selfconsistency' id='ref-wang2023selfconsistency-1'>
    11
</a> proposes sampling multiple outputs and choosing the most consistent answer from the set. One could also ask the <span class="caps">LLM</span> to produce multiple relevant recitations about the queries and use those recitations as context for the <span class="caps">LLM</span> to make a final output.
<figure>
<img src="images/2023-06-prompt-sampling.png" width="80%"/>
<figcaption align="center">
 Sampling multiple outputs <a href='#wang2023selfconsistency' id='ref-wang2023selfconsistency-2'>
    11
</a>
</figcaption>
</figure></p>
<p>The sampling and prompting techniques could be combined in any permuted order. For example, one could prompt the <span class="caps">LLM</span> to sample multiple recitations and keywords that describe the input query. The recitation passages could be deduplicated by or combined keywords before being used as examples in the final prompt fed to the <span class="caps">LLM</span>. In many ways, once an <span class="caps">LLM</span> completes learning what it could through its training stages. We can use prompt engineering techniques or modify the last layers to direct the <span class="caps">LLM</span> to squeeze information from its parameters and explore different outputs.</p>
<h4 id="retrieval-augmentation">Retrieval-Augmentation</h4>
<p>The easiest way to create a domain specific <span class="caps">AI</span> assistant is using a retrieval step to add a context to the prompt. This approach is commonly described as retriever-reader in the open book question-answering literature. This framework works well for <span class="caps">AI</span> assistants as well. The foundational model provides general reasoning and comprehension capabilities, and the information required to answer user queries is provided in the context. The key advantage of this approach is the retrieval step is very predictable and explicit. It is a much more efficient and reliable information retrieval procedure than prompting an <span class="caps">LLM</span> to find information stored in its parameters. However, the context window could become a limiting factor. The responses is likely to be superficial and read like a summary of the provided context. The <span class="caps">LLM</span> could not draw patterns, nor could it synthesize data from the entire corpus relevant to the query. The enterprise knowledge chatbot created by <a href="https://www.glean.com/blog/glean-chat-launch-announcement">Glean</a> follows this pattern.</p>
<p>The retrieval step is text search, which could use semantic embeddings powered by LLMs or vanilla lexical search. See my previous post on <a href="2023-04-04-document-search">document search</a>. There are additional ways to modify this setup. For example, <a href='#zhou2022ultron' id='ref-zhou2022ultron-1'>
    13
</a> proposes appending synthetic data to be indexed with the raw document to improve retrieval effectiveness.</p>
<h4 id="external-tools">External Tools</h4>
<p>The retrieval step is an example of using external tools to complement the knowledge implicitly stored by the <span class="caps">LLM</span>. OpenAI’s plugin and function calling <span class="caps">API</span> is another example of piecing together external tools to be part of an <span class="caps">AI</span> assistant. The basic framework is simple. Before the <span class="caps">LLM</span> produces the response, it determines whether it should perform external <span class="caps">API</span> queries. The set of available APIs could vary from application to application. The <span class="caps">LLM</span> can turn the original query into inputs to external APIs, and the <span class="caps">API</span>’s responses are converted into context as part of the final prompt. <a href='#schick2023toolformer' id='ref-schick2023toolformer-1'>
    7
</a> and <a href='#peng2023check' id='ref-peng2023check-1'>
    4
</a> test similar schemes to validate zero-shot performance across a variety of <span class="caps">NLP</span> tasks. </p>
<p>Such a component could be built and patched to any open-source LLMs. There are two key <span class="caps">NLP</span> tasks that this component requires. One is a classification task on whether the <span class="caps">LLM</span> needs to use the external <span class="caps">API</span> given a query. The other task is transforming the query into compatible <span class="caps">API</span> inputs. This framework extends the <span class="caps">AI</span> assistant’s ability to both retrieve relevant data and perform actions on behalf of the <span class="caps">AI</span> user.</p>
<h4 id="final-thoughts">Final Thoughts</h4>
<p><span class="caps">AI</span> assistants had not been very useful until the recent progress in LLMs. However, ChatGPT is just the beginning. I expect that more and more applications will enable meaningful and easy-to-use assistant user interface. These assistants won’t be general-purpose the same way that web search or ChatGPT are. These assistants will be limited in scope and domain-specific to the application. Chatbot style interaction is a natural extension to an search box, which has been a standard feature for the past decade. The next generation of applications will use chatbot as the interface for both information retrieval as well as performing actions.</p>
<hr/>
<h4 id="footnotes">Footnotes</h4>
<!-- 
more notes

https://github.com/zhaoxin94/awesome-domain-adaptation#theory -->


            <div id="citations">
    <hr>
    <h3>Citations</h3>
    <ol class="references">
            <li id="carlini2023quantifying">
                <span class="reference-text">Carlini, Nicholas, Ippolito, Daphne, Jagielski, Matthew, Lee, Katherine, Tramer, Florian, and Zhang, Chiyuan.
Quantifying memorization across neural language models.
2023.
<a href="https://arxiv.org/abs/2202.07646">arXiv:2202.07646</a>.</span>
                    <a class="cite-backref" href="#ref-carlini2023quantifying-1"
                       title="Jump back to reference 1">
                        <sup>
                            <i>
                                <b>
                                    1
                                </b>
                            </i>
                        </sup>
                    </a>
            </li>
            <li id="porada2021does">
                <span class="reference-text">Porada, Ian, Sordoni, Alessandro, and Cheung, Jackie Chi&nbsp;Kit.
Does pre-training induce systematic inference? how masked language models acquire commonsense knowledge.
2021.
<a href="https://arxiv.org/abs/2112.08583">arXiv:2112.08583</a>.</span>
                    <a class="cite-backref" href="#ref-porada2021does-1"
                       title="Jump back to reference 1">
                        <sup>
                            <i>
                                <b>
                                    1
                                </b>
                            </i>
                        </sup>
                    </a>
            </li>
            <li id="tay2023ul2">
                <span class="reference-text">Tay, Yi, Dehghani, Mostafa, Tran, Vinh&nbsp;Q., Garcia, Xavier, Wei, Jason, Wang, Xuezhi, Chung, Hyung&nbsp;Won, Shakeri, Siamak, Bahri, Dara, Schuster, Tal, Zheng, Huaixiu&nbsp;Steven, Zhou, Denny, Houlsby, Neil, and Metzler, Donald.
Ul2: unifying language learning paradigms.
2023.
<a href="https://arxiv.org/abs/2205.05131">arXiv:2205.05131</a>.</span>
                    <a class="cite-backref" href="#ref-tay2023ul2-1"
                       title="Jump back to reference 1">
                        <sup>
                            <i>
                                <b>
                                    1
                                </b>
                            </i>
                        </sup>
                    </a>
            </li>
            <li id="sun2021ernie">
                <span class="reference-text">Sun, Yu, Wang, Shuohuan, Feng, Shikun, Ding, Siyu, Pang, Chao, Shang, Junyuan, Liu, Jiaxiang, Chen, Xuyi, Zhao, Yanbin, Lu, Yuxiang, Liu, Weixin, Wu, Zhihua, Gong, Weibao, Liang, Jianzhong, Shang, Zhizhou, Sun, Peng, Liu, Wei, Ouyang, Xuan, Yu, Dianhai, Tian, Hao, Wu, Hua, and Wang, Haifeng.
Ernie 3.0: large-scale knowledge enhanced pre-training for language understanding and generation.
2021.
<a href="https://arxiv.org/abs/2107.02137">arXiv:2107.02137</a>.</span>
                    <a class="cite-backref" href="#ref-sun2021ernie-1"
                       title="Jump back to reference 1">
                        <sup>
                            <i>
                                <b>
                                    1
                                </b>
                            </i>
                        </sup>
                    </a>
            </li>
            <li id="moiseev2022skill">
                <span class="reference-text">Moiseev, Fedor, Dong, Zhe, Alfonseca, Enrique, and Jaggi, Martin.
Skill: structured knowledge infusion for large language models.
2022.
<a href="https://arxiv.org/abs/2205.08184">arXiv:2205.08184</a>.</span>
                    <a class="cite-backref" href="#ref-moiseev2022skill-1"
                       title="Jump back to reference 1">
                        <sup>
                            <i>
                                <b>
                                    1
                                </b>
                            </i>
                        </sup>
                    </a>
            </li>
            <li id="saadfalcon2023udapdr">
                <span class="reference-text">Saad-Falcon, Jon, Khattab, Omar, Santhanam, Keshav, Florian, Radu, Franz, Martin, Roukos, Salim, Sil, Avirup, Sultan, Md&nbsp;Arafat, and Potts, Christopher.
Udapdr: unsupervised domain adaptation via llm prompting and distillation of rerankers.
2023.
<a href="https://arxiv.org/abs/2303.00807">arXiv:2303.00807</a>.</span>
                    <a class="cite-backref" href="#ref-saadfalcon2023udapdr-1"
                       title="Jump back to reference 1">
                        <sup>
                            <i>
                                <b>
                                    1
                                </b>
                            </i>
                        </sup>
                    </a>
            </li>
            <li id="brown2020language">
                <span class="reference-text">Brown, Tom&nbsp;B., Mann, Benjamin, Ryder, Nick, Subbiah, Melanie, Kaplan, Jared, Dhariwal, Prafulla, Neelakantan, Arvind, Shyam, Pranav, Sastry, Girish, Askell, Amanda, Agarwal, Sandhini, Herbert-Voss, Ariel, Krueger, Gretchen, Henighan, Tom, Child, Rewon, Ramesh, Aditya, Ziegler, Daniel&nbsp;M., Wu, Jeffrey, Winter, Clemens, Hesse, Christopher, Chen, Mark, Sigler, Eric, Litwin, Mateusz, Gray, Scott, Chess, Benjamin, Clark, Jack, Berner, Christopher, McCandlish, Sam, Radford, Alec, Sutskever, Ilya, and Amodei, Dario.
Language models are few-shot learners.
2020.
<a href="https://arxiv.org/abs/2005.14165">arXiv:2005.14165</a>.</span>
                    <a class="cite-backref" href="#ref-brown2020language-1"
                       title="Jump back to reference 1">
                        <sup>
                            <i>
                                <b>
                                    1
                                </b>
                            </i>
                        </sup>
                    </a>
            </li>
            <li id="wei2023chainofthought">
                <span class="reference-text">Wei, Jason, Wang, Xuezhi, Schuurmans, Dale, Bosma, Maarten, Ichter, Brian, Xia, Fei, Chi, Ed, Le, Quoc, and Zhou, Denny.
Chain-of-thought prompting elicits reasoning in large language models.
2023.
<a href="https://arxiv.org/abs/2201.11903">arXiv:2201.11903</a>.</span>
                    <a class="cite-backref" href="#ref-wei2023chainofthought-1"
                       title="Jump back to reference 1">
                        <sup>
                            <i>
                                <b>
                                    1
                                </b>
                            </i>
                        </sup>
                    </a>
            </li>
            <li id="sun2023recitationaugmented">
                <span class="reference-text">Sun, Zhiqing, Wang, Xuezhi, Tay, Yi, Yang, Yiming, and Zhou, Denny.
Recitation-augmented language models.
2023.
<a href="https://arxiv.org/abs/2210.01296">arXiv:2210.01296</a>.</span>
                    <a class="cite-backref" href="#ref-sun2023recitationaugmented-1"
                       title="Jump back to reference 1">
                        <sup>
                            <i>
                                <b>
                                    1
                                </b>
                            </i>
                        </sup>
                    </a>
            </li>
            <li id="wang2023selfconsistency">
                <span class="reference-text">Wang, Xuezhi, Wei, Jason, Schuurmans, Dale, Le, Quoc, Chi, Ed, Narang, Sharan, Chowdhery, Aakanksha, and Zhou, Denny.
Self-consistency improves chain of thought reasoning in language models.
2023.
<a href="https://arxiv.org/abs/2203.11171">arXiv:2203.11171</a>.</span>
                    <a class="cite-backref" href="#ref-wang2023selfconsistency-1"
                       title="Jump back to reference 1">
                        <sup>
                            <i>
                                <b>
                                    1
                                </b>
                            </i>
                        </sup>
                    </a>
                    <a class="cite-backref" href="#ref-wang2023selfconsistency-2"
                       title="Jump back to reference 2">
                        <sup>
                            <i>
                                <b>
                                    2
                                </b>
                            </i>
                        </sup>
                    </a>
            </li>
            <li id="zhou2022ultron">
                <span class="reference-text">Zhou, Yujia, Yao, Jing, Dou, Zhicheng, Wu, Ledell, Zhang, Peitian, and Wen, Ji-Rong.
Ultron: an ultimate retriever on corpus with a model-based indexer.
2022.
<a href="https://arxiv.org/abs/2208.09257">arXiv:2208.09257</a>.</span>
                    <a class="cite-backref" href="#ref-zhou2022ultron-1"
                       title="Jump back to reference 1">
                        <sup>
                            <i>
                                <b>
                                    1
                                </b>
                            </i>
                        </sup>
                    </a>
            </li>
            <li id="schick2023toolformer">
                <span class="reference-text">Schick, Timo, Dwivedi-Yu, Jane, Dessì, Roberto, Raileanu, Roberta, Lomeli, Maria, Zettlemoyer, Luke, Cancedda, Nicola, and Scialom, Thomas.
Toolformer: language models can teach themselves to use tools.
2023.
<a href="https://arxiv.org/abs/2302.04761">arXiv:2302.04761</a>.</span>
                    <a class="cite-backref" href="#ref-schick2023toolformer-1"
                       title="Jump back to reference 1">
                        <sup>
                            <i>
                                <b>
                                    1
                                </b>
                            </i>
                        </sup>
                    </a>
            </li>
            <li id="peng2023check">
                <span class="reference-text">Peng, Baolin, Galley, Michel, He, Pengcheng, Cheng, Hao, Xie, Yujia, Hu, Yu, Huang, Qiuyuan, Liden, Lars, Yu, Zhou, Chen, Weizhu, and Gao, Jianfeng.
Check your facts and try again: improving large language models with external knowledge and automated feedback.
2023.
<a href="https://arxiv.org/abs/2302.12813">arXiv:2302.12813</a>.</span>
                    <a class="cite-backref" href="#ref-peng2023check-1"
                       title="Jump back to reference 1">
                        <sup>
                            <i>
                                <b>
                                    1
                                </b>
                            </i>
                        </sup>
                    </a>
            </li>
    </ol>
</div>

             
 
            
            
            








            <hr/>
            <script src="https://utteranc.es/client.js"
                    repo="jinfwhuang/jinfwhuang.github.io"
                    issue-term="pathname"
                    label="user-comments"
                    theme="github-light"
                    crossorigin="anonymous"
                    async>
            </script>

            <hr/>
<section>
    <h2>Related Posts</h2>
<ul class="related-posts-list">
<li><a href="/2023-04-04-document-search" title="Pretrained LLMs and Text Search - A practitioner&#39;s perspective">Pretrained LLMs and Text Search <small>A practitioner's perspective</small></a></li>
<li><a href="/2023-04-27-open-source-llm" title="Open Source LLMs">Open Source LLMs</a></li>
</ul>
<hr />
</section>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="/2023-04-27-open-source-llm" title="Previous: Open Source LLMs">Open Source LLMs</a></li>
                <li class="next-article"><a href="/2023-06-25-wing-foiling-tips" title="Next: First Notes on Wing Foiling - I am still early in the process">First Notes on Wing Foiling <small class="subtitle">I am still early in the process</small></a> »</li>
            </ul>
            </nav>
            </aside>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2023-06-04T00:00:00-07:00">Sun 04 June 2023</time>
            <!--             <h4>Category</h4>
            <a class="category-link" href="/categories#misc-ref">misc</a>
 -->
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="/tags#ai-ref">ai
                    <span class="superscript">3</span>
</a></li>
                <li><a href="/tags#llm-ref">LLM
                    <span class="superscript">3</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="https://twitter.com/jinfwhuang" title="Twiiter" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Twitter" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
    </a>
    <a href="https://www.linkedin.com/in/jinfwhuang" title="LinkedIn" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="LinkedIn" role="img" viewBox="0 0 512 512" fill="#fff"><rect width="512" height="512" rx="15%" fill="#0077b5"/><circle cx="142" cy="138" r="37"/><path stroke="#fff" stroke-width="66" d="M244 194v198M142 194v198"/><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<!--        <footer>

    <div>
        <span class="site-name"><span style="color:black;">Jin's Notes</span></span> - the hardest part is taking the first step
    </div>



    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>-->
            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>




    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->

</html>