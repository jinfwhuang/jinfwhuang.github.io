<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="jin" />

        <meta name="description" content="Preliminaries In this blog post, I discuss down my observations on the throughputs of some of the major blockchains: Ethereum 2, Cosmo, Polkadot, and Solana. I choose to use byte throughout, byte/s, as a key metric to comparing different blockchains instead of transactions, tx/s. Most of of scalability …
" />
        <meta name="twitter:creator" content="@jinfwhuang">
        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="web3, misc, " />

<meta property="og:title" content="Notes on Blockchain Throughput "/>
<meta property="og:url" content="/2021-07-23-blockchain_throughput" />
<meta property="og:description" content="Preliminaries In this blog post, I discuss down my observations on the throughputs of some of the major blockchains: Ethereum 2, Cosmo, Polkadot, and Solana. I choose to use byte throughout, byte/s, as a key metric to comparing different blockchains instead of transactions, tx/s. Most of of scalability …" />
<meta property="og:site_name" content="Jin&#39;s Notes" />
<meta property="og:article:author" content="jin" />
<meta property="og:article:published_time" content="2021-07-23T00:00:00-07:00" />
<meta name="twitter:title" content="Notes on Blockchain Throughput ">
<meta name="twitter:description" content="Preliminaries In this blog post, I discuss down my observations on the throughputs of some of the major blockchains: Ethereum 2, Cosmo, Polkadot, and Solana. I choose to use byte throughout, byte/s, as a key metric to comparing different blockchains instead of transactions, tx/s. Most of of scalability …">
<meta property="og:image" content="/images/android-chrome-192x192.png" />
<meta name="twitter:image" content="/images/android-chrome-192x192.png" >

        <title>Notes on Blockchain Throughput  · Jin&#39;s Notes
</title>
        <link rel="shortcut icon" href="/theme/images/favicon.ico" type="image/x-icon" />
        <link rel="icon" href="/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="/theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="/theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="/theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="/theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="/theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="/theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="/theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="/theme/images/apple-touch-icon-180x180.png" type="image/png" />
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-207279664-1', 'auto');
    ga('send', 'pageview');
</script>


    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="/"><span class=site-name><span style="color:black;">Jin's Notes</span></span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       "/"
                                    >Home</a>
                                </li>
<!--                                <li ><a href="/categories">Categories</a></li>-->
                                <li ><a href="/tags">Tags</a></li>
                                <li ><a href="/archives">Archives</a></li>
                                <li><form class="navbar-search" action="/search" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="/2021-07-23-blockchain_throughput">
                Notes on Blockchain Throughput <br/>
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
    <div class="span2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div class="toc">
<ul>
<li><a href="#preliminaries">Preliminaries</a></li>
<li><a href="#eth2">Eth2</a><ul>
<li><a href="#shard-capacity">Shard Capacity</a></li>
<li><a href="#sharding-limit-and-shard-network">Sharding Limit and Shard Network</a></li>
</ul>
</li>
<li><a href="#cosmo">Cosmo</a></li>
<li><a href="#polkadot">Polkadot</a></li>
<li><a href="#solana">Solana</a></li>
<li><a href="#final-words">Final Words</a></li>
</ul>
</div>
        </nav>
    </div>
    <div class="span8 article-content">
            
            
<h2 id="preliminaries">Preliminaries</h2>
<p>In this blog post, I discuss down my observations on the throughputs of some of the major blockchains: Ethereum 2, Cosmo, Polkadot, and Solana.</p>
<p>I choose to use byte throughout, byte/s, as a key metric to comparing different blockchains instead of transactions, tx/s. Most of of scalability comparisons focus on transaction throughput. See <a href="https://solana.com/solana-whitepaper.pdf">ex1</a>, <a href="https://towardsdatascience.com/the-blockchain-scalability-problem-the-race-for-visa-like-transaction-speed-5cce48f9d44">ex2</a>, or <a href="https://ieeexplore.ieee.org/ielx7/6287639/8948470/09133427.pdf?tp=&amp;arnumber=9133427&amp;isnumber=8948470&amp;ref=aHR0cHM6Ly9tZWRpdW0uY29tL2Jsb2NrY2hhaW4tc2NhbGFiaWxpdHkvYmxvY2tjaGFpbi1zY2FsYWJpbGl0eS04ZmU3YTQwYTk0MTQ=">ex3</a>. Transactions per second could be a misleading metric because different data models and layer 2 solutions could lead to dramatically different byte size per transaction. For example, Ethereum transactions averaged to about 500 bytes. If a blockchain only uses a 8 byte address space and take advantage of aggregate signatures to limit signature size to 2 byte, a transaction would be 10 bytes. For another example, state channels could also be used to put data off-chain to achieve scalability. Byte is a more generic measure to compare blockchain throughputs.</p>
<p>Data on a public ledger could be used to encode vastly different messages, ranging from simple token transfers to arbitrary computations. Transaction data in Bitcoin is primarily used to do accounting for unspent transaction outputs. A general purpose blockchain such as Ethereum could use data to encode computations via op codes and contract states. Measuring a blockchain’s throughput in bytes allows us to ignore the blockchain’s computing paradigm, allowing us to focus on how its consensus mechanism and p2p network architecture limit what a blockchain could do.</p>
<p>Bitcoin’s block size is <span class="caps">1MB</span> and there about 150 blocks per day. Bitcoin’s data throughput is about <strong>1.5 <span class="caps">KB</span>/s</strong>. Ethereum <strong>3.5 <span class="caps">KB</span>/s</strong>. The two blockchain’s data rates are remarkably similar. See <a href="https://ycharts.com/indicators/ethereum_average_block_size">Ethereum Average Block Size</a> and <a href="https://ycharts.com/indicators/ethereum_blocks_per_day">Ethereum Blocks Per Day</a>. </p>
<p>Different data rates support different applications. For example, much of the decentralized finance applications are similar in that they record infrequent buying and selling activities of some blockchain assets. Assume that I make 100 transactions a year, with each transaction being about 100 bytes. I only need to <span class="caps">10KB</span> of data. If there are 10 million people using the platform, 100 <span class="caps">GB</span>/year is required. That is about <strong>4 <span class="caps">KB</span>/s</strong>. Ethereum could support this type of applications as of today. In fact, defi applications are thriving in Ethereum today.</p>
<p>An application that uses more data would be a widely payment system. Visa is the probably the most frequently cited example showing the inadequate throughput of Bitcoin and Ethereum. Assuming Visa payment system processes 10,000 tx/s, it would be about <strong>1 <span class="caps">MB</span>/s</strong>. This far exceeds the current 1-4 <span class="caps">KB</span>/s range in Ethereum. But it is insurmountable. We are likely to get 10-50x improvement in Eth2. If we get another performance boost through layer 2 technologies, it is not inconceivable that we will get to the payment system throughput requirement. I will discuss this later in the post.</p>
<p>Gaming, messaging, social networking, or online advertising system have a much higher data throughput requirement. I would want to envision a decentralized internet where my data is not owned by a centralized private company. Let us assume that an average American uses 100 <span class="caps">KB</span> per day in emails, textings, and social media platforms. Assume 300 million people. The platform would have to support <strong>300 <span class="caps">MB</span>/s</strong>. That sounds a lot, but there are a lot of enterprise applications that easily process data volume much greater than that. It will be hard for any blockchain to support this type of data throughput. I will look into details later in this blog post to explain why. Applications in this category will likely use off-chain data processing and storage techniques. It is a useful reminder that decentralization does not all data and computation have to be pushed on-chain.</p>
<h2 id="eth2">Eth2</h2>
<p>Eth2 has by far the most detailed designs and open discussions of the all the major blockchains. The proposed specs shed the most light on what blockchain platforms will look like in the near future.</p>
<p>Eth2 is a sharded blockchain. The initial target is 64 shards. The other important proposed targets are block size of 128 <span class="caps">KB</span> and a block time of 12 seconds. See <a href="https://github.com/ethereum/Eth2.0-specs/blob/dev/configs/mainnet.yaml">mainnet spec</a>, <a href="https://github.com/ethereum/Eth2.0-specs/blob/dev/specs/sharding/beacon-chain.md">beacon spec</a>, and <a href="https://notes.ethereum.org/@vbuterin/HkiULaluS">note on shard chain proposal</a>. The two key metrics that determine data throughput is the data capacity of one shard and the upper limit of the number of shards that the Eth2 could support.</p>
<h4 id="shard-capacity">Shard Capacity</h4>
<p>The targeted throughput of one shard is roughly <strong>10 <span class="caps">KB</span>/s</strong>. This target is already well calibrated for various theoretical and practical reasons. Vitalik has a good <a href="https://vitalik.ca/general/2021/05/23/scaling.html">post</a> explaining how the practical details of internet bandwidth, computing power, and storage dictate the limit the data throughput of a single shard. I do not fully agree with Vitalik’s stated limit on bandwidth because requiring a gigabit bandwidth does not increase centralization risk. There are sufficient people who have residential gigabit or are able to leverage cloud resources. The accumulated size the chain state is a hard limit. It is not inconceivable that network participants are required to have large storage devices. The users could have 10 <span class="caps">TB</span> of <span class="caps">SSD</span> storage to store active data. For simplicity of argument, assume that the most recent blocks from the last 3 years are active data, and older blockers could be put into deep, networked storage. This would lead to a throughput of 100 <span class="caps">KB</span>/s. With such a large state db, where the hot data spans 10 <span class="caps">TB</span>, the best we could do to provide fast access to the state db is through memory mapped indices. Even assuming that the nodes are required to have 64 <span class="caps">GB</span> of main memory, majority of reads and writes would still require the system to swap pages. Even if all other validation steps are instantaneous, reading state itself slows down the event processing. There is sufficient evidences in the last decade of enterprise experiences from data queries and stream processing frameworks that a database processing node with 10 <span class="caps">TB</span> of hot data is unlikely to perform well. If the chain accumulates about 1 <span class="caps">TB</span> of data over 3 years, the throughput is about 10 <span class="caps">KB</span>/s.</p>
<p>It should be noted that all of those limits could be increased. Instead of thinking the participating validator as a single machine, we could design that to be a processing cluster. The validator runs a cluster of workers to have both distributed storage and stream processing. However, this would dramatically increase centralization risk. The participants are likely to be enterprise users or highly motivated group of individuals. It would be easier for them to find each other and collude to compromise the integrity of the network.</p>
<p>It cannot be overstated the importance of decentralization. If data is stored and processed only by a few large well-known participants, the overall ecosystem will emerge to look similar to today’s internet. Decentralization requires the barriers of entry on participating in core network to be as small as possible. This requirement leads and some heuristics about shard throughput. For more details, see a post on <a href="https://vitalik.ca/general/2021/04/07/sharding.html">scalability dilemma</a></p>
<p>It is also important to point out that the limit on per shard throughput is not due to specifics of consensus mechanisms. A consensus mechanics limits the latency (block time) of when data is processed, but it does not limit the targeted block size. For a fixed block time, block size is limited by the data throughput of the validating nodes.</p>
<p>One misunderstanding about overcoming the per shard capacity is that one could further shard the shards. That is a misguided solution. A transaction processing node in the network has an upper limit in its ability to hold a portion of the chain’s state and process the assigned transactions. The appropriate partitioning of the network lead to a sharding design. The network could add more shards or split the shards further, but the network does not uniformly benefit further from a two level sharding design unless there are specific justified transaction network topologies. For example, a layer 1 shard could correspond to countries, and a layer 2 shard could correspond to cities within countries. This type of network design is not likely to be general purpose. I will talk about transaction network topology in the next section.</p>
<h4 id="sharding-limit-and-shard-network">Sharding Limit and Shard Network</h4>
<p>The primary shard is called the Beacon Chain, which supports the consensus mechanism. There is fixed amount of work that the Beacon Chain has to perform to keep tab of validators, proposer committees, block headers, etc. If shards do not talk to each other, the per shard data on the Beacon Chain is minimum. The Beacon Chain should be able to support thousands of shards if the only data being put on Beacon is block headers and other administrative accountings. However, cross-shard communication is a requirement for Eth2.</p>
<p>The size of Beacon Chain scales linearly with the number of transactions that have cross-shard communications. Cross-shard linkage is still a work in progress. See <a href="https://eth.wiki/en/sharding/sharding-roadmap">roadmap</a>, <a href="https://notes.ethereum.org/@serenity/H1PGqDhpm?type=view">sharding research note</a>, and <a href="https://ethresear.ch/t/implementing-cross-shard-transactions/6382">research discussion</a>. One implementation of cross-shard transactions could be aggregating log receipts in  Beacon. Beacon would grow with the number of cross-shard transactions. If the fraction of cross-shard transactions is relatively stable, there would be a natural limit to the number of shards that Beacon could support. If the majority of the transactions are cross-shard, effectively Beacon has to make available all of those data. Jamming all the sharded data into one shard would quickly overwhelm the Beacon shard. The number of shards that Beacon could support is inversely proportionate to the fraction of transactions that are cross-shard. For example, if 1% of the transactions are cross-shard, we could expect Beacon to support 100 shards.</p>
<p>Combining the heuristics of one shard throughput and shard number limit, it would suggest that a reasonable byte throughput upper bound is <strong>1 <span class="caps">MB</span>/s</strong> if the cross-shard rate is 1%. If the cross-shard rate is 0.1%, and we would get <strong>10 <span class="caps">MB</span>/s</strong>. Referencing our discussions earlier, it would mean that a global payment system could be implemented on chain. But we won’t see micropayment system or social networks implemented directly on Eth2. Those applications will require layer 2 scaling solutions.</p>
<p>The bottleneck of cross-shard communications puts an upper bound on the connectedness of the sharded topology. Shard to shard communications could not exceed the limit of a per shard capacity. For example, a global payment system is implemented on chain. Each zipcode’s internal payments are placed in one shard. If the payment going inside and outside of that zipcode far exceeds the remaining capacity of the shard, the payment system gets overloaded. An application designer has to identify the eventual network architecture and understands if the bottleneck could be eliminated by grouping those users to a shard. It is possible that transaction topology is sufficiently connected that there is no sharding solution available given the per shard throughput. For those who are interested, I wrote a paper a long time ago on <a href="docs/bottleneck_may2013">network bottleneck</a> that had more details on how to identify and calculate network hotspots.</p>
<h2 id="cosmo">Cosmo</h2>
<p>Cosmos is a decentralized network of independent parallel blockchains. Each cosmos-backed blockchain is secured by the Tindermint Byzantine Fault Tolerant (<span class="caps">BFT</span>) consensus. Application developers use the Cosmo <span class="caps">SDK</span> to write their own data model and state transition logic. A blockchain could exist completely independent of the rest of the Cosmo network. Each Cosmos blockchain is called a zone, and there are special Cosmos chains called hubs that coordinate cross-chain communications through <a href="https://github.com/cosmos/ibc/raw/old/papers/2020-05/build/paper.pdf"><span class="caps">IBC</span></a>. Each zone has its own governance to decide how the validations are performed. The hub does not validate any transactions coming from zones.</p>
<p>Each zone is essentially is a parallel chain. The throughput of each zone falls under a similar set of scalability requirements of a single Ethereum shard, <strong>10 <span class="caps">KB</span>/s</strong>. If transactions do not have to be cross-zone, we could create as many parallel zones as possible for different applications. As long as there is no single application that exceeds the capacity of a single zone, there is no limit on throughput. </p>
<p>However, an example application such as a global payment system would require many cross-chain communications. Such an application would have to designed with many zones and many hubs. Because cross-chain communications could be designed in any zone-hub connection, a payment application could divide users appropriately into different interconnected zone. The theoretical limit would be whether the overall network could support all the cross-chain links. For an extreme example, if users in the payment network form a complete network and there are frequent cross-links, even if each person takes up all the capacity of a dedicated zone and hub, the number of cross-links would overwhelm the dedicated hub. Most real life networks would be much more sparse. Still, application developers would have to be clever about managing sharding the users to different zones. This is no different than designing a sharded database in centralized web technologies. The key difference is that per zone capacity is small, and each zone’s cross-chain throughput could only take up a fraction of that.</p>
<h2 id="polkadot">Polkadot</h2>
<p><a href="https://polkadot.network/PolkaDotPaper.pdf">Polkadot</a> uses a master Relay Chain to coordinate consensus. Transactions are finalized when they are ordered by the master chain. The system has a universal set of validators that are assigned to validate each of the parachains. Validators do not store state information, but they store block headers and block proofs. Validators are capable of ensuring that the blocks are valid without the need to build out the chain’s history or current state. Each parachain has a set of Collators. Collators maintain the full data that is used to construct the state transitions for their assigned parachain. Collators generate the blocks and provide validity proofs of the generated blocks. </p>
<p>The per node storage and computing throughput of Collators limits the throughput of a parachain. Each collator has to store the full parachain. This means that each parachain faces the same shard limit as a single Eth2 shard assuming Collators use similar hardwares as Eth2 validators. Validators collectively ensure block proofs are available through erasure coding. Assuming block proof is 1000th of the block size, then it is unlikely that storing block proofs will be a bottleneck because each validator would only need to store 1000th many bytes to a Collator, multiplying the shard number, which is 100. Hence a parachain would process about 10 <span class="caps">KB</span>/s. In Polkadot’s proposal, it suggests that Collators are expected to run with enterprise level hardwares and network connections. We could expect the actual throughput to be multiple of that, gaining that at the expense of higher centralization risk.</p>
<p>Polkadot enables direct cross-chain interaction through cross-chain message passing (xcmp). The content of cross chain messages do not get to Relay Chain, but the metadata of the messages are included in the Relay Chain. Cross-chain messaging implies two limits. First, the cross-chain messages cannot exceed the capacity of a single parachain. Second, the combined metadata of all cross-chain messages on all parachains cannot exceed the capacity of Relay Chain. This is similar to Eth2 Beacon Chain. The exception is that Beacon Chain might include the actual cross-shard messages, and Polkadot Relay Chain only include metadata. The fundamental limit is similar: the combined cross-shard traffic capacity is the throughput of one single shard. For this reason and probably also due to other nuances, the number of parachain is limited to 100. I would expect the Polkadot throughput to be around <strong>1-20 <span class="caps">MB</span>/s</strong>.</p>
<h2 id="solana">Solana</h2>
<p><a href="https://solana.com/solana-whitepaper.pdf">Solana</a> is not proposed to be a sharded blockchain, at least not as of the writing of this post. Solana aims to push the blockchain’s throughput for a single machine. It has a high hardware requirements for validator. It recommends validator to have 16 cores, 256 <span class="caps">GB</span> of memory, TBs of <span class="caps">SSD</span> disk storage, and gigabit network.</p>
<p>Solana aims to obtain a transaction throughput that matches a network’s gigabit speed, 125 <span class="caps">MB</span>/s. It combines a number of techniques to remove network, consensus, and storage bottlenecks. The network has a concept of slot and epoch. An slot is about 400 ms, and an epoch is 432,000 slots, which is roughly 2-3 days. A leader is deterministically chosen every epoch. Its transaction forwarding protocol <a href="https://medium.com/solana-labs/gulf-stream-solanas-mempool-less-transaction-forwarding-protocol-d342e72186ad">Gulf Stream</a> ensures that the network could quickly route the transactions to the leader. The leader uses <a href="https://medium.com/solana-labs/proof-of-history-a-clock-for-blockchain-cf47a61a9274">proof of history</a> to order all the transactions. <a href="https://medium.com/solana-labs/tower-bft-solanas-high-performance-implementation-of-pbft-464725911e79">Tower <span class="caps">BFT</span></a> allows validators to quickly vote on the blocks. Fork choice rule is a result of proof of history and tower <span class="caps">BFT</span>. Furthermore, its <a href="https://medium.com/solana-labs/turbine-solanas-block-propagation-protocol-solves-the-scalability-trilemma-2ddba46a51db">turbine gossip</a> protocol allows the out-going messages from the leader to reach validators without breaking the 125 <span class="caps">MB</span>/s network bottleneck. Furthermore, Solana proposes an archiving system to allow validator nodes to shed historical data to a storage network. These innovations are clever. It is believable that they could achieve the stated throughput at the network, consensus, and storage layer. That is, this throughput is possible if the state machine’s operations from processing each tx are trivial operations.</p>
<p>Processing transactions requires reading from and writing to the state. At 125 <span class="caps">MB</span>/s, it is about 4 <span class="caps">PB</span> per year. In theory, all of this data could be stateful updates. For example, if each transaction is a simple transaction writing key-value pair where the key is random and the value is strictly increasing of all existing values. The state size would grow quickly and easily exceed the limit of a single machine. Solana should not be able to handle the large state without distributing the reading and writing of the state, which are in the petabyte scale. The 125 <span class="caps">MB</span>/s throughput is only possible if almost all of the transaction data does not lead to stateful updates. This data processing problem is not unique to blockchain. Many enterprise companies have built data streaming processing framework to tackle similar problems. A processing system has to be able to split the work and coordinate stateful updates.</p>
<p>As mentioned before, Eth2’s data throughput is not limited by the shortcoming of its consensus mechanism. The 15 seconds block time in Eth1 is due to proof of work. The difficulty is adjusted so that blocks are created roughly once over that period. There is empirical research to show that <a href="https://sites.cs.ucsb.edu/~rich/class/cs293b-cloud/papers/bitcoin-delay">12 seconds delay</a> is needed for the network to propagate and verify the transactions in a large decentralized system such as Bitcoin and Etheruem. The gossiping mechanism for propagating proposed blocks is slower than Solana’s approach. That slows down blocktime but it is not a bottleneck for the overall data throughput. There are a number of heuristics that could reduce the targeted block time: smaller blocks, separate block headers and block contents (see this <a href="https://blog.ethereum.org/2014/07/11/toward-a-12-second-block-time">post</a>), etc. The true limit of data throughput is state size and transaction history, which is a result of storage requirement.</p>
<p>Solana proposes key innovations to reduce bottlenecks around network, consensus, and storage. I do not believe that Solana could handle the theoretical <span class="caps">125MB</span>/s throughput for arbitrary transaction data. Let’s assume that a Solana validator could safely support a state size of 3 <span class="caps">TB</span> and that is accumulated in 3 years, and on averaged, 10% of the transaction data become the state. Solana would be able to support about 1 <span class="caps">MB</span>/s. That is still really impressive throughput.</p>
<h2 id="final-words">Final Words</h2>
<p>The throughput of a single shard blockchain is limited. This limit could be increased if each validator effectively runs a distributed computing and a storage cluster, making one shard a network of clusters. Assuming that per shard throughput is fixed, a network of shards increases throughput linearly. A sharded blockchain could be bottlenecked by cross-shard communications. If an application could tolerate a network topology where there are small complete networks encapsulated within shards and there are only sparse cross-shard linkage, the application could be supported on chain. A sharded blockchain could support throughput up to <strong>1-20 <span class="caps">MB</span>/s</strong> while allowing reasonable cross-shard linkages. This would be sufficient to support a global payment system. Data heavy applications such as gaming, messaging, and social media will likely have to leverage layer 2 techniques.</p>



             
 
            
            
            








            <hr/>
            <script src="https://utteranc.es/client.js"
                    repo="jinfwhuang/jinfwhuang.github.io"
                    issue-term="pathname"
                    label="user-comments"
                    theme="github-light"
                    crossorigin="anonymous"
                    async>
            </script>

            <hr/>
<section>
    <h2>Related Posts</h2>
<ul class="related-posts-list">
<li><a href="/2021-09-06-dsn" title="Notes on Decentralized Storage Networks">Notes on Decentralized Storage Networks</a></li>
<li><a href="/2022-03-15-stablecoins" title="Notes on ERC20 Stablecoins">Notes on ERC20 Stablecoins</a></li>
<li><a href="/2021-03-22-ethereum-l2" title="Notes on Ethereum L2 Solutions">Notes on Ethereum L2 Solutions</a></li>
<li><a href="/2022-09-01-nft-platform" title="Notes on NFT Platforms">Notes on NFT Platforms</a></li>
<li><a href="/2022-10-01-scaling-nft-metadata" title="Scaling NFT Metadata">Scaling NFT Metadata</a></li>
</ul>
<hr />
</section>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="/2021-01-31-quote-of-the-month" title="Previous: Quote of the Month">Quote of the Month</a></li>
                <li class="next-article"><a href="/2021-09-06-dsn" title="Next: Notes on Decentralized Storage Networks">Notes on Decentralized Storage Networks</a> »</li>
            </ul>
            </nav>
            </aside>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2021-07-23T00:00:00-07:00">Fri 23 July 2021</time>
            <!--             <h4>Category</h4>
            <a class="category-link" href="/categories#misc-ref">misc</a>
 -->
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="/tags#web3-ref">web3
                    <span class="superscript">6</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="https://twitter.com/jinfwhuang" title="Twiiter" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Twitter" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
    </a>
    <a href="https://www.linkedin.com/in/jinfwhuang" title="LinkedIn" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="LinkedIn" role="img" viewBox="0 0 512 512" fill="#fff"><rect width="512" height="512" rx="15%" fill="#0077b5"/><circle cx="142" cy="138" r="37"/><path stroke="#fff" stroke-width="66" d="M244 194v198M142 194v198"/><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<!--        <footer>

    <div>
        <span class="site-name"><span style="color:black;">Jin's Notes</span></span> - the hardest part is taking the first step
    </div>



    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>-->
            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>




    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->

</html>